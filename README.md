# LoRA from Scratch

LoRA, which stands for Low-Rank Adaptation, is a popular technique to finetune LLMs more efficiently. Instead of adjusting all the parameters of a deep neural network, LoRA focuses on updating only a small set of low-rank matrices. 

By reducing the computational load, LoRA allows for the adaptation of these models on a much smaller scale of computing resources, making the use of large models more attainable for researchers and organizations with limited resources.




