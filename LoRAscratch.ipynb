{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In practice, to equip and finetune a model with LoRA,\\n all we have to do is replace its pretrained Linear layers \\nwith our new LinearWithLoRA layer.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        #standart deviation of weights. we use sqrt(1/n) <--> tanh activat.func\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = torch.nn.Parameter(torch.randn(in_dim, rank)*std_dev)\n",
    "        #at the beginning of the training, before A and B are updated via backpropagation, \n",
    "        #the LoRALayer does not impact the original weights because AB=0 if B=0.\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        #magnitude of the changes introduced by the LoRA layer to the model's existing weights\n",
    "        self.alpha = alpha\n",
    "        #A higher value of alpha means larger adjustments to the model's behavior, \n",
    "        # while a lower value results in more subtle changes.\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "#replace each Linear layer with a LinearWithLoRA layer that combines the Linear layer with Lora\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear=linear\n",
    "        self.lora=LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "\n",
    "\"\"\"In practice, to equip and finetune a model with LoRA,\n",
    " all we have to do is replace its pretrained Linear layers \n",
    "with our new LinearWithLoRA layer.\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning with LoRA \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train a small BERT model for text classification\n",
    "we will use a pretrained DistilBERT (a smaller version of BERT) model from the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want to train the new LoRA weights, we freeze all model parameters by setting `requires_grad` to `False` for all trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 66,955,010\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "print(f\"Total number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
      "distilbert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "distilbert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "distilbert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "distilbert.transformer.layer.0.attention.q_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.0.attention.q_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.0.attention.v_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.0.attention.v_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight torch.Size([768])\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias torch.Size([768])\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight torch.Size([3072, 768])\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias torch.Size([3072])\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight torch.Size([768, 3072])\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias torch.Size([768])\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight torch.Size([768])\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias torch.Size([768])\n",
      "distilbert.transformer.layer.1.attention.q_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.1.attention.q_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.1.attention.k_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.1.attention.k_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.1.attention.v_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.1.attention.v_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.1.attention.out_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.1.attention.out_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.1.sa_layer_norm.weight torch.Size([768])\n",
      "distilbert.transformer.layer.1.sa_layer_norm.bias torch.Size([768])\n",
      "distilbert.transformer.layer.1.ffn.lin1.weight torch.Size([3072, 768])\n",
      "distilbert.transformer.layer.1.ffn.lin1.bias torch.Size([3072])\n",
      "distilbert.transformer.layer.1.ffn.lin2.weight torch.Size([768, 3072])\n",
      "distilbert.transformer.layer.1.ffn.lin2.bias torch.Size([768])\n",
      "distilbert.transformer.layer.1.output_layer_norm.weight torch.Size([768])\n",
      "distilbert.transformer.layer.1.output_layer_norm.bias torch.Size([768])\n",
      "distilbert.transformer.layer.2.attention.q_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.2.attention.q_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.2.attention.k_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.2.attention.k_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.2.attention.v_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.2.attention.v_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.2.attention.out_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.2.attention.out_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.2.sa_layer_norm.weight torch.Size([768])\n",
      "distilbert.transformer.layer.2.sa_layer_norm.bias torch.Size([768])\n",
      "distilbert.transformer.layer.2.ffn.lin1.weight torch.Size([3072, 768])\n",
      "distilbert.transformer.layer.2.ffn.lin1.bias torch.Size([3072])\n",
      "distilbert.transformer.layer.2.ffn.lin2.weight torch.Size([768, 3072])\n",
      "distilbert.transformer.layer.2.ffn.lin2.bias torch.Size([768])\n",
      "distilbert.transformer.layer.2.output_layer_norm.weight torch.Size([768])\n",
      "distilbert.transformer.layer.2.output_layer_norm.bias torch.Size([768])\n",
      "distilbert.transformer.layer.3.attention.q_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.3.attention.q_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.3.attention.k_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.3.attention.k_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.3.attention.v_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.3.attention.v_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.3.attention.out_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.3.attention.out_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.3.sa_layer_norm.weight torch.Size([768])\n",
      "distilbert.transformer.layer.3.sa_layer_norm.bias torch.Size([768])\n",
      "distilbert.transformer.layer.3.ffn.lin1.weight torch.Size([3072, 768])\n",
      "distilbert.transformer.layer.3.ffn.lin1.bias torch.Size([3072])\n",
      "distilbert.transformer.layer.3.ffn.lin2.weight torch.Size([768, 3072])\n",
      "distilbert.transformer.layer.3.ffn.lin2.bias torch.Size([768])\n",
      "distilbert.transformer.layer.3.output_layer_norm.weight torch.Size([768])\n",
      "distilbert.transformer.layer.3.output_layer_norm.bias torch.Size([768])\n",
      "distilbert.transformer.layer.4.attention.q_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.4.attention.q_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.4.attention.k_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.4.attention.k_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.4.attention.v_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.4.attention.v_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.4.attention.out_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.4.attention.out_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.4.sa_layer_norm.weight torch.Size([768])\n",
      "distilbert.transformer.layer.4.sa_layer_norm.bias torch.Size([768])\n",
      "distilbert.transformer.layer.4.ffn.lin1.weight torch.Size([3072, 768])\n",
      "distilbert.transformer.layer.4.ffn.lin1.bias torch.Size([3072])\n",
      "distilbert.transformer.layer.4.ffn.lin2.weight torch.Size([768, 3072])\n",
      "distilbert.transformer.layer.4.ffn.lin2.bias torch.Size([768])\n",
      "distilbert.transformer.layer.4.output_layer_norm.weight torch.Size([768])\n",
      "distilbert.transformer.layer.4.output_layer_norm.bias torch.Size([768])\n",
      "distilbert.transformer.layer.5.attention.q_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.5.attention.q_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.5.attention.k_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.5.attention.k_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.5.attention.v_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.5.attention.v_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.5.attention.out_lin.weight torch.Size([768, 768])\n",
      "distilbert.transformer.layer.5.attention.out_lin.bias torch.Size([768])\n",
      "distilbert.transformer.layer.5.sa_layer_norm.weight torch.Size([768])\n",
      "distilbert.transformer.layer.5.sa_layer_norm.bias torch.Size([768])\n",
      "distilbert.transformer.layer.5.ffn.lin1.weight torch.Size([3072, 768])\n",
      "distilbert.transformer.layer.5.ffn.lin1.bias torch.Size([3072])\n",
      "distilbert.transformer.layer.5.ffn.lin2.weight torch.Size([768, 3072])\n",
      "distilbert.transformer.layer.5.ffn.lin2.bias torch.Size([768])\n",
      "distilbert.transformer.layer.5.output_layer_norm.weight torch.Size([768])\n",
      "distilbert.transformer.layer.5.output_layer_norm.bias torch.Size([768])\n",
      "pre_classifier.weight torch.Size([768, 768])\n",
      "pre_classifier.bias torch.Size([768])\n",
      "classifier.weight torch.Size([2, 768])\n",
      "classifier.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66955010"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict = [p.numel() for p in model.parameters()]\n",
    "param_dict\n",
    "#sum(param_dict)\n",
    "sum(param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 66955010\n",
      "Trainable Parameters: 0\n",
      "Non-trainable Parameters: 66955010\n"
     ]
    }
   ],
   "source": [
    "#numel() returns a total number of element in tensor\n",
    "# Calculating the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Trainable Parameters: {trainable_params}\") # We set requires_grad = True to apply lora\n",
    "print(f\"Non-trainable Parameters: {non_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "layer1: LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ") assigned with LoRA\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "layer1: LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ") assigned with LoRA\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "layer2: LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ") assigned with LoRA\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "layer2: LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ") assigned with LoRA\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "layer3: LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ") assigned with LoRA\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "layer3: LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ") assigned with LoRA\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "layer4: LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ") assigned with LoRA\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "layer4: LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ") assigned with LoRA\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "layer5: LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ") assigned with LoRA\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "layer5: LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ") assigned with LoRA\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "layer6: LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ") assigned with LoRA\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "layer6: LinearWithLoRA(\n",
      "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (lora): LoRALayer()\n",
      ") assigned with LoRA\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "#default hyhperparam choices\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "#Dropout is a regularization technique that randomly sets \n",
    "#some of the activations to zero during training,\n",
    "#which can help prevent overfitting. The default value is 0.05.\n",
    "lora_dropout = 0.05\n",
    "\n",
    "lora_query = True #whether the query matrix is adapted\n",
    "lora_key = False #whether the key matrix is adapted\n",
    "lora_value = True #whether the value matrix is adapted\n",
    "lora_projection = False # whether the projection matrices\n",
    "lora_mlp= False #MLP \n",
    "lora_head = False # whether the final classification head is adapted\n",
    "\n",
    "layers = []\n",
    "\n",
    "assign_lora = partial(LinearWithLoRA, rank= lora_r, alpha = lora_alpha)\n",
    "\n",
    "for i, layer in enumerate(model.distilbert.transformer.layer):\n",
    "    if lora_query:\n",
    "        print(\"-----\"*25)\n",
    "        layer.attention.q_lin = assign_lora(layer.attention.q_lin)\n",
    "        print(f\"layer{i+1}: {layer.attention.q_lin} assigned with LoRA\")\n",
    "    if lora_key:\n",
    "        print(\"-----\"*25)\n",
    "        layer.attention.k_lin = assign_lora(layer.attention.k_lin)\n",
    "        print(f\"layer{i+1}: {layer.attention.k_lin} assigned with LoRA\")\n",
    "    if lora_value:\n",
    "        print(\"-----\"*25)\n",
    "        layer.attention.v_lin = assign_lora(layer.attention.v_lin)\n",
    "        print(f\"layer{i+1}: {layer.attention.v_lin} assigned with LoRA\")\n",
    "    if lora_projection:\n",
    "        print(\"-----\"*25)\n",
    "        layer.attention.out_lin = assign_lora(layer.attention.out_lin)\n",
    "        print(f\"layer{i+1}: {layer.attention.out_lin} assigned with LoRA\")\n",
    "    if lora_mlp:\n",
    "        \n",
    "        layer.ffn.lin1 = assign_lora(layer.ffn.lin1)\n",
    "        print(f\"layer{i+1}: {layer.ffn.lin1 } assigned with LoRA\")\n",
    "        print(\"-----\"*25)\n",
    "        layer.ffn.lin2 = assign_lora(layer.ffn.lin2)\n",
    "        print(f\"layer{i+1}: {layer.ffn.lin2 } assigned with LoRA\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect the model again to check its updated structure using print(model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model with the default hyperparameter choices above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATASET - IMDb Movie Reviews classification dataset: https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.36.2)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: lightning in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: watermark in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.4.3)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lightning) (0.10.0)\n",
      "Requirement already satisfied: torch<4.0,>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lightning) (2.1.1+cu121)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lightning) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lightning) (4.9.0)\n",
      "Requirement already satisfied: pytorch-lightning in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from lightning) (2.1.2)\n",
      "Requirement already satisfied: ipython>=6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from watermark) (8.17.2)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from watermark) (7.0.1)\n",
      "Requirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from watermark) (68.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from importlib-metadata>=1.4->watermark) (3.17.0)\n",
      "Requirement already satisfied: decorator in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.0->watermark) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.0->watermark) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.0->watermark) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.0->watermark) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.0->watermark) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.0->watermark) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.0->watermark) (5.14.1)\n",
      "Requirement already satisfied: exceptiongroup in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.0->watermark) (1.0.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from ipython>=6.0->watermark) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (3.1.3)\n",
      "Requirement already satisfied: triton==2.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.0->watermark) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.0->watermark) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.0->watermark) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.12.0->lightning) (2.1.3)\n",
      "Requirement already satisfied: executing in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython>=6.0->watermark) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython>=6.0->watermark) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stack-data->ipython>=6.0->watermark) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.12.0->lightning) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets lightning watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch       : 2.1.1+cu121\n",
      "transformers: 4.36.2\n",
      "datasets    : 2.16.1\n",
      "lightning   : 2.1.2\n",
      "\n",
      "conda environment: n/a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark --conda -p torch,transformers,datasets,lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Loading the dataset into DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from local_dataset_utilities import download_dataset, load_dataset_into_to_dataframe, partition_dataset\n",
    "from local_dataset_utilities import IMDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Please switch to a GPU machine before running this notebook.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:51<00:00, 967.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n"
     ]
    }
   ],
   "source": [
    "files = {\"test.csv\", \"train.csv\", \"val.csb\"}\n",
    "download = True\n",
    "\n",
    "for f in files:\n",
    "        if not os.path.exists(os.path.join(\"data\", f)):\n",
    "            download = False\n",
    "\n",
    "if download is False:\n",
    "    download_dataset()\n",
    "    df = load_dataset_into_to_dataframe()\n",
    "    partition_dataset(df) # returns \"data/val.csv & data/train.csv & data/test.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(\"data\", \"val.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(\"data\", \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>When we started watching this series on cable,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Steve Biko was a black activist who tried to r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>My short comment for this flick is go pick it ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>As a serious horror fan, I get that certain ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Robert Cummings, Laraine Day and Jean Muir sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34995</th>\n",
       "      <td>0</td>\n",
       "      <td>Frank Capra's creativity must have been just a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34996</th>\n",
       "      <td>0</td>\n",
       "      <td>Just saw the film tonight in a preview and it'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34997</th>\n",
       "      <td>0</td>\n",
       "      <td>If you love Japanese monster movies, you'll lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34998</th>\n",
       "      <td>0</td>\n",
       "      <td>Because it came from HBO and based on the IMDb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34999</th>\n",
       "      <td>0</td>\n",
       "      <td>WARNING!!! SOME POSSIBLE PLOT SPOILERS, AS IF ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                               text  label\n",
       "0          0  When we started watching this series on cable,...      1\n",
       "1          0  Steve Biko was a black activist who tried to r...      1\n",
       "2          0  My short comment for this flick is go pick it ...      1\n",
       "3          0  As a serious horror fan, I get that certain ma...      0\n",
       "4          0  Robert Cummings, Laraine Day and Jean Muir sta...      1\n",
       "...      ...                                                ...    ...\n",
       "34995      0  Frank Capra's creativity must have been just a...      0\n",
       "34996      0  Just saw the film tonight in a preview and it'...      0\n",
       "34997      0  If you love Japanese monster movies, you'll lo...      1\n",
       "34998      0  Because it came from HBO and based on the IMDb...      0\n",
       "34999      0  WARNING!!! SOME POSSIBLE PLOT SPOILERS, AS IF ...      0\n",
       "\n",
       "[35000 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2 Tokenization and Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset with load_dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 35000\n",
      "    })\n",
      "    validatiaon: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset =load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\" : os.path.join(\"data\", \"train.csv\"),\n",
    "        \"validatiaon\" : os.path.join(\"data\", \"val.csv\"),\n",
    "        \"test\" : os.path.join(\"data\", \"test.csv\")\n",
    "    },\n",
    ")\n",
    "\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43208877ae4548e4b48b67ae99667cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3545f834b7cf462c96626ec0a79825a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067771039c7541179fee9a58ce2a34d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_tokenized = imdb_dataset.map(tokenize_text, batched = True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    validatiaon: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    validatiaon: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Set Up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset_dict, partition_key=\"train\"):\n",
    "        self.partition = dataset_dict[partition_key]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.partition[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.partition.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validatiaon\")\n",
    "test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD LoRA layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1/torch.sqrt(torch.tensor(rank).float())\n",
    "        self.W_a = torch.nn.Parameter(torch.randn(in_dim, rank)*std_dev)\n",
    "        self.W_b = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha *( x@ self.W_a @ self.W_b)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear \n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "lora_query = True\n",
    "lora_key = False\n",
    "lora_value = True\n",
    "lora_projection = False\n",
    "lora_mlp = False\n",
    "lora_head = False\n",
    "\n",
    "layers = []\n",
    "\n",
    "assign_lora = partial(LinearWithLoRA, rank=lora_r, alpha = lora_alpha)\n",
    "\n",
    "for layer in model.distilbert.transformer.layer:\n",
    "    if lora_query:\n",
    "        layer.attention.q_lin = assign_lora(layer.attention.q_lin)\n",
    "    if lora_key:\n",
    "        layer.attention.k_lin = assign_lora(layer.attention.k_lin)\n",
    "    if lora_value:\n",
    "        layer.attention.v_lin = assign_lora(layer.attention.v_lin)\n",
    "    if lora_projection:\n",
    "        layer.attention.out_lin = assign_lora(layer.attention.out_lin)\n",
    "    if lora_mlp:\n",
    "        layer.ffn.lin1 = assign_lora(layer.ffn.lin1)\n",
    "        layer.ffn.lin2 = assign_lora(layer.ffn.lin2)\n",
    "if lora_head:\n",
    "    model.pre_classifier = assign_lora(model.pre_classifier)\n",
    "    model.classifier = assign_lora(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): LinearWithLoRA(\n",
       "              (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): LinearWithLoRA(\n",
       "              (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.embeddings.word_embeddings.weight : False\n",
      "distilbert.embeddings.position_embeddings.weight : False\n",
      "distilbert.embeddings.LayerNorm.weight : False\n",
      "distilbert.embeddings.LayerNorm.bias : False\n",
      "distilbert.transformer.layer.0.attention.q_lin.linear.weight : False\n",
      "distilbert.transformer.layer.0.attention.q_lin.linear.bias : False\n",
      "distilbert.transformer.layer.0.attention.q_lin.lora.W_a : True\n",
      "distilbert.transformer.layer.0.attention.q_lin.lora.W_b : True\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight : False\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias : False\n",
      "distilbert.transformer.layer.0.attention.v_lin.linear.weight : False\n",
      "distilbert.transformer.layer.0.attention.v_lin.linear.bias : False\n",
      "distilbert.transformer.layer.0.attention.v_lin.lora.W_a : True\n",
      "distilbert.transformer.layer.0.attention.v_lin.lora.W_b : True\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight : False\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias : False\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight : False\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias : False\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight : False\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias : False\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight : False\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias : False\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight : False\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias : False\n",
      "distilbert.transformer.layer.1.attention.q_lin.linear.weight : False\n",
      "distilbert.transformer.layer.1.attention.q_lin.linear.bias : False\n",
      "distilbert.transformer.layer.1.attention.q_lin.lora.W_a : True\n",
      "distilbert.transformer.layer.1.attention.q_lin.lora.W_b : True\n",
      "distilbert.transformer.layer.1.attention.k_lin.weight : False\n",
      "distilbert.transformer.layer.1.attention.k_lin.bias : False\n",
      "distilbert.transformer.layer.1.attention.v_lin.linear.weight : False\n",
      "distilbert.transformer.layer.1.attention.v_lin.linear.bias : False\n",
      "distilbert.transformer.layer.1.attention.v_lin.lora.W_a : True\n",
      "distilbert.transformer.layer.1.attention.v_lin.lora.W_b : True\n",
      "distilbert.transformer.layer.1.attention.out_lin.weight : False\n",
      "distilbert.transformer.layer.1.attention.out_lin.bias : False\n",
      "distilbert.transformer.layer.1.sa_layer_norm.weight : False\n",
      "distilbert.transformer.layer.1.sa_layer_norm.bias : False\n",
      "distilbert.transformer.layer.1.ffn.lin1.weight : False\n",
      "distilbert.transformer.layer.1.ffn.lin1.bias : False\n",
      "distilbert.transformer.layer.1.ffn.lin2.weight : False\n",
      "distilbert.transformer.layer.1.ffn.lin2.bias : False\n",
      "distilbert.transformer.layer.1.output_layer_norm.weight : False\n",
      "distilbert.transformer.layer.1.output_layer_norm.bias : False\n",
      "distilbert.transformer.layer.2.attention.q_lin.linear.weight : False\n",
      "distilbert.transformer.layer.2.attention.q_lin.linear.bias : False\n",
      "distilbert.transformer.layer.2.attention.q_lin.lora.W_a : True\n",
      "distilbert.transformer.layer.2.attention.q_lin.lora.W_b : True\n",
      "distilbert.transformer.layer.2.attention.k_lin.weight : False\n",
      "distilbert.transformer.layer.2.attention.k_lin.bias : False\n",
      "distilbert.transformer.layer.2.attention.v_lin.linear.weight : False\n",
      "distilbert.transformer.layer.2.attention.v_lin.linear.bias : False\n",
      "distilbert.transformer.layer.2.attention.v_lin.lora.W_a : True\n",
      "distilbert.transformer.layer.2.attention.v_lin.lora.W_b : True\n",
      "distilbert.transformer.layer.2.attention.out_lin.weight : False\n",
      "distilbert.transformer.layer.2.attention.out_lin.bias : False\n",
      "distilbert.transformer.layer.2.sa_layer_norm.weight : False\n",
      "distilbert.transformer.layer.2.sa_layer_norm.bias : False\n",
      "distilbert.transformer.layer.2.ffn.lin1.weight : False\n",
      "distilbert.transformer.layer.2.ffn.lin1.bias : False\n",
      "distilbert.transformer.layer.2.ffn.lin2.weight : False\n",
      "distilbert.transformer.layer.2.ffn.lin2.bias : False\n",
      "distilbert.transformer.layer.2.output_layer_norm.weight : False\n",
      "distilbert.transformer.layer.2.output_layer_norm.bias : False\n",
      "distilbert.transformer.layer.3.attention.q_lin.linear.weight : False\n",
      "distilbert.transformer.layer.3.attention.q_lin.linear.bias : False\n",
      "distilbert.transformer.layer.3.attention.q_lin.lora.W_a : True\n",
      "distilbert.transformer.layer.3.attention.q_lin.lora.W_b : True\n",
      "distilbert.transformer.layer.3.attention.k_lin.weight : False\n",
      "distilbert.transformer.layer.3.attention.k_lin.bias : False\n",
      "distilbert.transformer.layer.3.attention.v_lin.linear.weight : False\n",
      "distilbert.transformer.layer.3.attention.v_lin.linear.bias : False\n",
      "distilbert.transformer.layer.3.attention.v_lin.lora.W_a : True\n",
      "distilbert.transformer.layer.3.attention.v_lin.lora.W_b : True\n",
      "distilbert.transformer.layer.3.attention.out_lin.weight : False\n",
      "distilbert.transformer.layer.3.attention.out_lin.bias : False\n",
      "distilbert.transformer.layer.3.sa_layer_norm.weight : False\n",
      "distilbert.transformer.layer.3.sa_layer_norm.bias : False\n",
      "distilbert.transformer.layer.3.ffn.lin1.weight : False\n",
      "distilbert.transformer.layer.3.ffn.lin1.bias : False\n",
      "distilbert.transformer.layer.3.ffn.lin2.weight : False\n",
      "distilbert.transformer.layer.3.ffn.lin2.bias : False\n",
      "distilbert.transformer.layer.3.output_layer_norm.weight : False\n",
      "distilbert.transformer.layer.3.output_layer_norm.bias : False\n",
      "distilbert.transformer.layer.4.attention.q_lin.linear.weight : False\n",
      "distilbert.transformer.layer.4.attention.q_lin.linear.bias : False\n",
      "distilbert.transformer.layer.4.attention.q_lin.lora.W_a : True\n",
      "distilbert.transformer.layer.4.attention.q_lin.lora.W_b : True\n",
      "distilbert.transformer.layer.4.attention.k_lin.weight : False\n",
      "distilbert.transformer.layer.4.attention.k_lin.bias : False\n",
      "distilbert.transformer.layer.4.attention.v_lin.linear.weight : False\n",
      "distilbert.transformer.layer.4.attention.v_lin.linear.bias : False\n",
      "distilbert.transformer.layer.4.attention.v_lin.lora.W_a : True\n",
      "distilbert.transformer.layer.4.attention.v_lin.lora.W_b : True\n",
      "distilbert.transformer.layer.4.attention.out_lin.weight : False\n",
      "distilbert.transformer.layer.4.attention.out_lin.bias : False\n",
      "distilbert.transformer.layer.4.sa_layer_norm.weight : False\n",
      "distilbert.transformer.layer.4.sa_layer_norm.bias : False\n",
      "distilbert.transformer.layer.4.ffn.lin1.weight : False\n",
      "distilbert.transformer.layer.4.ffn.lin1.bias : False\n",
      "distilbert.transformer.layer.4.ffn.lin2.weight : False\n",
      "distilbert.transformer.layer.4.ffn.lin2.bias : False\n",
      "distilbert.transformer.layer.4.output_layer_norm.weight : False\n",
      "distilbert.transformer.layer.4.output_layer_norm.bias : False\n",
      "distilbert.transformer.layer.5.attention.q_lin.linear.weight : False\n",
      "distilbert.transformer.layer.5.attention.q_lin.linear.bias : False\n",
      "distilbert.transformer.layer.5.attention.q_lin.lora.W_a : True\n",
      "distilbert.transformer.layer.5.attention.q_lin.lora.W_b : True\n",
      "distilbert.transformer.layer.5.attention.k_lin.weight : False\n",
      "distilbert.transformer.layer.5.attention.k_lin.bias : False\n",
      "distilbert.transformer.layer.5.attention.v_lin.linear.weight : False\n",
      "distilbert.transformer.layer.5.attention.v_lin.linear.bias : False\n",
      "distilbert.transformer.layer.5.attention.v_lin.lora.W_a : True\n",
      "distilbert.transformer.layer.5.attention.v_lin.lora.W_b : True\n",
      "distilbert.transformer.layer.5.attention.out_lin.weight : False\n",
      "distilbert.transformer.layer.5.attention.out_lin.bias : False\n",
      "distilbert.transformer.layer.5.sa_layer_norm.weight : False\n",
      "distilbert.transformer.layer.5.sa_layer_norm.bias : False\n",
      "distilbert.transformer.layer.5.ffn.lin1.weight : False\n",
      "distilbert.transformer.layer.5.ffn.lin1.bias : False\n",
      "distilbert.transformer.layer.5.ffn.lin2.weight : False\n",
      "distilbert.transformer.layer.5.ffn.lin2.bias : False\n",
      "distilbert.transformer.layer.5.output_layer_norm.weight : False\n",
      "distilbert.transformer.layer.5.output_layer_norm.bias : False\n",
      "pre_classifier.weight : False\n",
      "pre_classifier.bias : False\n",
      "classifier.weight : False\n",
      "classifier.bias : False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} : {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 147456\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of trainable parameters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_model_utilities import CustomLightningModule\n",
    "\n",
    "lightning_model = CustomLightningModule(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        save_top_k=1, mode = \"max\", monitor = \"val_acc\"\n",
    "    )\n",
    "]\n",
    "logger = CSVLogger(save_dir = \"logs/\", name = \"my-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A10G') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: logs/my-model\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                                | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model    | DistilBertForSequenceClassification | 67.1 M\n",
      "1 | val_acc  | MulticlassAccuracy                  | 0     \n",
      "2 | test_acc | MulticlassAccuracy                  | 0     \n",
      "-----------------------------------------------------------------\n",
      "147 K     Trainable params\n",
      "67.0 M    Non-trainable params\n",
      "67.1 M    Total params\n",
      "268.410   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f997ba6ac2d4c3f95a19c92b4f968ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6419e019d5d140029e98defbc36eee9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf58abed115843edbe76cacb4d803ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9c04af3fe94c3d886e0ae1e253458d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ba7b019f8b4d08938c3c694c7b0d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "trainer.fit(model = lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders = val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf3b84acb0848a192757b6128b353ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=0-step=2917.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c46c333d4f4090a6ea8c9d446a5d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=0-step=2917.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e225a787fb547849f585af8b8f57407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc = trainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\", verbose=False)\n",
    "val_acc = trainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\", verbose=False)\n",
    "test_acc = trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 89.96%\n",
      "Val acc:   89.18%\n",
      "Test acc:  87.73%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train acc: {train_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print(f\"Val acc:   {val_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print(f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Cleanup checkpoint files as we don't need them later\n",
    "log_dir = f\"logs/my-model\"\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
